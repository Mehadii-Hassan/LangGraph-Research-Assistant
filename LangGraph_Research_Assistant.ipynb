{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVJFKkXJaSkn",
        "outputId": "ab03cb69-f111-482d-b8c3-8d49656c5a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langgraph langchain langchain-community langchain-core langchain-openai chromadb pypdf sentence-transformers langchain-ollama\n",
        "print(\"Packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    !sudo apt-get install -y pciutils\n",
        "    !curl -fsSL https://ollama.com/install.sh | sh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ontPBeELaV_M",
        "outputId": "43feaa90-59b6-4e57-ea6d-d434013dc66c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Run Ollama server in background\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "# Pull the embedding model (run once)\n",
        "!ollama pull all-minilm\n",
        "\n",
        "print(\"Ollama installed and running! Model pulled.\")\n",
        "# Wait for server to start (optional sleep)\n",
        "import time\n",
        "time.sleep(10)  # Give time for startup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txzaNMUSaV8a",
        "outputId": "f6d5d0c6-be8b-4c9b-cb29-b48a721a2429"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "Ollama installed and running! Model pulled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    import subprocess\n",
        "    import os\n",
        "\n",
        "    def start_ollama_server():\n",
        "        os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
        "        os.environ['OLLAMA_ORIGINS'] = '*'\n",
        "        subprocess.Popen([\"ollama\", \"serve\"])\n",
        "        print(\"ðŸš€ Ollama server launched successfully!\")\n",
        "\n",
        "    start_ollama_server()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__6_qzKGaV5u",
        "outputId": "2bf2c683-c687-48c4-b9e0-c70d3b88a9ca"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Ollama server launched successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Cell 2: Imports & Config ==========\n",
        "import os\n",
        "import uuid\n",
        "from typing import TypedDict, Annotated, List, Dict, Any\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Global in-memory cache for session vectorstores\n",
        "SESSION_PDF_STORES: dict = {}\n",
        "\n",
        "# --- Keep your token lines unchanged as requested ---\n",
        "os.environ[\"GITHUB_TOKEN\"] = \"your_github_api_key\"\n",
        "token = os.environ.get(\"GITHUB_TOKEN\")\n",
        "if not token:\n",
        "    raise ValueError(\"GITHUB_TOKEN not set.\")\n",
        "\n",
        "# Configure LLM (use your provided base_url/model)\n",
        "llm = ChatOpenAI(\n",
        "    api_key=token,\n",
        "    base_url=\"https://models.github.ai/inference\",\n",
        "    model=\"openai/gpt-4.1-nano\",\n",
        "    temperature=0.0\n",
        ")\n",
        "\n",
        "# Configure embeddings (Ollama)\n",
        "embeddings = OllamaEmbeddings(model=\"all-minilm\")\n",
        "\n",
        "print(\"LLM and Embeddings configured successfully!\")\n",
        "\n",
        "# Sanity tests (non-fatal)\n",
        "try:\n",
        "    test_response = llm.invoke(\"Hello, what is 2+2?\").content\n",
        "    print(\"LLM Test Response:\", test_response)\n",
        "except Exception as e:\n",
        "    print(\"LLM test failed (ok if service not reachable):\", e)\n",
        "\n",
        "try:\n",
        "    embedding_test = embeddings.embed_query(\"Test embedding\")\n",
        "    print(\"Embeddings Test: Success! Vector length:\", len(embedding_test))\n",
        "except Exception as e:\n",
        "    print(\"Embeddings Error (ok if Ollama not running locally):\", str(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZyaWnvVaV3M",
        "outputId": "10856c5b-7ab5-4e09-b572-22e386359381"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM and Embeddings configured successfully!\n",
            "LLM Test Response: Hello! 2 + 2 equals 4.\n",
            "Embeddings Test: Success! Vector length: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Cell 3: State & Long-term memory setup ==========\n",
        "class ResearchState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], \"Conversation history (short-term memory)\"]\n",
        "    pdf_paths: Annotated[List[str], \"Paths to uploaded PDFs (session-local)\"]\n",
        "    pdf_chunks: Annotated[List[str], \"Chunked text from the uploaded PDFs (session-local)\"]\n",
        "    retrieved_docs: Annotated[List[str], \"Retrieved relevant documents from PDF or long-term memory\"]\n",
        "    long_term_insights: Annotated[List[str], \"New insights to store in long-term memory after response\"]\n",
        "    conversation_count: Annotated[int, \"Number of exchanges in current session\"]\n",
        "    session_id: Annotated[str, \"Unique session identifier\"]  # Explicitly define in state\n",
        "\n",
        "# Persistent long-term memory directory\n",
        "long_term_db_path = \"/content/long_term_memory\"\n",
        "os.makedirs(long_term_db_path, exist_ok=True)\n",
        "\n",
        "# Initialize persistent Chroma for long-term memory (embedding_function kw kept)\n",
        "long_term_vectorstore = Chroma(\n",
        "    collection_name=\"long_term_insights\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=long_term_db_path\n",
        ")\n",
        "\n",
        "def persist_long_term():\n",
        "    \"\"\"Persist long-term store if supported; tolerate deprecation/auto-persist differences.\"\"\"\n",
        "    try:\n",
        "        if hasattr(long_term_vectorstore, \"persist\"):\n",
        "            long_term_vectorstore.persist()\n",
        "            print(\"Long-term memory persisted to:\", long_term_db_path)\n",
        "        else:\n",
        "            print(\"Long-term memory auto-persist or persist not available in this Chroma version.\")\n",
        "    except Exception as e:\n",
        "        print(\"Note: persistence call issued but raised:\", e)\n",
        "\n",
        "print(\"Long-term memory initialized at:\", long_term_db_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj17bIEkaV0g",
        "outputId": "ff334e82-26ec-4af1-8d5e-0fe9169e6433"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Long-term memory initialized at: /content/long_term_memory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Cell 4: PDF Processing & Summarization Helpers ==========\n",
        "def process_pdf_paths(pdf_paths: List[str], chunk_size: int = 1000, chunk_overlap: int = 200):\n",
        "    \"\"\"\n",
        "    Load multiple PDFs and chunk them. Return chunk texts only.\n",
        "    (Session vectorstores are created separately and cached globally.)\n",
        "    \"\"\"\n",
        "    all_chunks: List[str] = []\n",
        "    for pdf_path in pdf_paths:\n",
        "        try:\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            documents = loader.load()\n",
        "            text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "            chunks = text_splitter.split_documents(documents)\n",
        "            chunk_texts = [d.page_content for d in chunks]\n",
        "            all_chunks.extend(chunk_texts)\n",
        "            print(f\"Processed {pdf_path}: {len(chunk_texts)} chunks\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {pdf_path}: {e}\")\n",
        "    return all_chunks\n",
        "\n",
        "\n",
        "def summarize_text_with_llm(text: str, max_sentences: int = 5) -> str:\n",
        "    \"\"\"\n",
        "    Use the LLM to produce a concise summary.\n",
        "    \"\"\"\n",
        "    prompt_template = (\n",
        "        \"Summarize the following text into up to {n} short, numbered sentences. Keep each sentence concise.\\n\\n\"\n",
        "        \"Text:\\n{txt}\\n\\nSummary:\"\n",
        "    )\n",
        "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    try:\n",
        "        summary = chain.invoke({\"n\": max_sentences, \"txt\": text})\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        return \"Summary failed: \" + str(e)"
      ],
      "metadata": {
        "id": "jFOz_mbcaVx4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Cell 5: LangGraph Node Implementations ==========\n",
        "def load_pdfs_node(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"\n",
        "    Load and chunk all PDFs for this session; create a cached session Chroma store and\n",
        "    ensure session_id is preserved from initial_state.\n",
        "    \"\"\"\n",
        "    pdf_paths = state.get(\"pdf_paths\", []) or []\n",
        "    # Ensure session_id is present; raise error if not provided in initial_state\n",
        "    ### MODIFIED: Removed fallback session_id creation to enforce initial_state provision\n",
        "    session_id = state.get(\"session_id\")\n",
        "    if not session_id:\n",
        "        raise ValueError(\"session_id must be provided in initial_state\")\n",
        "\n",
        "    if not pdf_paths:\n",
        "        print(\"No PDFs provided in this session.\")\n",
        "        SESSION_PDF_STORES.setdefault(session_id, None)\n",
        "        return {**state, \"pdf_chunks\": [], \"pdf_paths\": pdf_paths, \"session_pdf_store_present\": False}\n",
        "\n",
        "    # Produce chunk texts\n",
        "    chunks = process_pdf_paths(pdf_paths)\n",
        "    state = {**state, \"pdf_chunks\": chunks, \"pdf_paths\": pdf_paths}\n",
        "\n",
        "    # Create session vectorstore if not already cached\n",
        "    if SESSION_PDF_STORES.get(session_id) is None and chunks:\n",
        "        try:\n",
        "            session_store = Chroma(collection_name=f\"session_pdf_{session_id}\", embedding_function=embeddings)\n",
        "            session_store.add_texts(chunks)\n",
        "            SESSION_PDF_STORES[session_id] = session_store\n",
        "            print(\"Session PDF vectorstore created and cached (via add_texts).\")\n",
        "            state[\"session_pdf_store_present\"] = True\n",
        "        except Exception as e:\n",
        "            print(\"Warning: failed to create session PDF vectorstore (embedding service might be offline):\", e)\n",
        "            SESSION_PDF_STORES[session_id] = None\n",
        "            state[\"session_pdf_store_present\"] = False\n",
        "    else:\n",
        "        state[\"session_pdf_store_present\"] = bool(SESSION_PDF_STORES.get(session_id))\n",
        "        if state[\"session_pdf_store_present\"]:\n",
        "            print(\"Using pre-existing session vectorstore from cache.\")\n",
        "\n",
        "    print(f\"load_pdfs_node: loaded {len(chunks)} chunks across {len(pdf_paths)} PDF(s). session_id={session_id}\")\n",
        "    return state\n",
        "\n",
        "\n",
        "def analyze_query_node(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"Analyze query (extract keywords, intent) using LLM.\"\"\"\n",
        "    last_message = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Analyze this user query and return comma-separated keywords and a one-line intent summary.\\n\\n\"\n",
        "        \"Query: {query}\\n\\nOutput format: keywords: kw1, kw2; intent: one-line summary\"\n",
        "    )\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    try:\n",
        "        analysis = chain.invoke({\"query\": last_message})\n",
        "        state[\"analysis\"] = analysis\n",
        "        print(\"Query analysis:\", analysis)\n",
        "    except Exception as e:\n",
        "        state[\"analysis\"] = f\"Analysis failed: {e}\"\n",
        "        print(\"Analysis failed:\", e)\n",
        "    return state\n",
        "\n",
        "\n",
        "def retrieve_info_node(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"Retrieve relevant chunks from session PDFs and long-term memory.\"\"\"\n",
        "    query = state[\"messages\"][-1].content if state[\"messages\"] else \"\"\n",
        "    retrieved_texts: List[str] = []\n",
        "\n",
        "    ### MODIFIED: Added session_id check and removed ephemeral vectorstore creation\n",
        "    session_id = state.get(\"session_id\")\n",
        "    if not session_id:\n",
        "        raise ValueError(\"session_id missing in state\")\n",
        "\n",
        "    session_pdf_store = SESSION_PDF_STORES.get(session_id) if session_id else None\n",
        "\n",
        "    # Query short-term/session store\n",
        "    if session_pdf_store:\n",
        "        try:\n",
        "            pdf_hits = session_pdf_store.similarity_search(query, k=3)\n",
        "            retrieved_texts.extend([doc.page_content for doc in pdf_hits])\n",
        "            print(f\"Retrieved {len(pdf_hits)} docs from session PDFs.\")\n",
        "        except Exception as e:\n",
        "            print(\"Session PDF retrieval error:\", e)\n",
        "    else:\n",
        "        print(\"Warning: No session vectorstore found for session_id:\", session_id)\n",
        "\n",
        "    # Query long-term store\n",
        "    try:\n",
        "        lt_hits = long_term_vectorstore.similarity_search(query, k=3)\n",
        "        retrieved_texts.extend([doc.page_content for doc in lt_hits])\n",
        "        print(f\"Retrieved {len(lt_hits)} docs from long-term memory.\")\n",
        "    except Exception as e:\n",
        "        print(\"Long-term retrieval error (ok if empty):\", e)\n",
        "\n",
        "    return {**state, \"retrieved_docs\": retrieved_texts, \"conversation_count\": state.get(\"conversation_count\", 0) + 1}\n",
        "\n",
        "\n",
        "def generate_response_node(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"Generate an answer: fuse history + retrieved docs + query.\"\"\"\n",
        "    query = state[\"messages\"][-1].content\n",
        "    context = \"\\n\\n---\\n\\n\".join(state.get(\"retrieved_docs\", [])[:6])  # cap context length\n",
        "    history = \"\\n\".join([msg.content for msg in state[\"messages\"][-6:]])  # keep last 6 messages\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are a research assistant. Use the context and conversation history to answer the user's query.\\n\\n\"\n",
        "        \"If the answer is not in the provided context, be honest and say you couldn't find it locally, \"\n",
        "        \"but suggest reasonable next steps (e.g., which sections to check in the paper).\\n\\n\"\n",
        "        \"Provide citations when possible in the form: (source: session_pdf_chunk_<n> or long_term_insight).\\n\\n\"\n",
        "        \"Query: {query}\\n\\nContext: {context}\\n\\nHistory: {history}\\n\\nAnswer:\"\n",
        "    )\n",
        "    prompt = ChatPromptTemplate.from_template(system_prompt)\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    try:\n",
        "        response = chain.invoke({\"query\": query, \"context\": context, \"history\": history})\n",
        "    except Exception as e:\n",
        "        response = \"LLM generation failed: \" + str(e)\n",
        "\n",
        "    ai_msg = AIMessage(content=response)\n",
        "    return {**state, \"messages\": state[\"messages\"] + [ai_msg]}\n",
        "\n",
        "\n",
        "def store_insights_node(state: ResearchState) -> ResearchState:\n",
        "    \"\"\"Extract concise insights from the last AI response and store to long-term memory.\"\"\"\n",
        "    if not state.get(\"messages\"):\n",
        "        return state\n",
        "\n",
        "    last_response = state[\"messages\"][-1].content\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"Extract 1-3 concise, standalone insights from this assistant response. Output bullet lines separated by newline.\\n\\nResponse:\\n{response}\\n\\nInsights:\"\n",
        "    )\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    try:\n",
        "        insights_text = chain.invoke({\"response\": last_response})\n",
        "        insights = [s.strip() for s in insights_text.splitlines() if s.strip()][:3]\n",
        "    except Exception as e:\n",
        "        print(\"Insight extraction failed:\", e)\n",
        "        insights = []\n",
        "\n",
        "    if insights:\n",
        "        try:\n",
        "            long_term_vectorstore.add_texts(insights)\n",
        "            persist_long_term()\n",
        "            print(f\"Stored {len(insights)} insight(s) to long-term memory.\")\n",
        "        except Exception as e:\n",
        "            print(\"Failed to store insights:\", e)\n",
        "\n",
        "    return {**state, \"long_term_insights\": insights}"
      ],
      "metadata": {
        "id": "pNZjKGUcaVvT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Cell 6: Build the LangGraph workflow ==========\n",
        "def create_research_graph():\n",
        "    workflow = StateGraph(ResearchState)\n",
        "\n",
        "    workflow.add_node(\"load_pdfs\", load_pdfs_node)\n",
        "    workflow.add_node(\"analyze_query\", analyze_query_node)\n",
        "    workflow.add_node(\"retrieve_info\", retrieve_info_node)\n",
        "    workflow.add_node(\"generate_response\", generate_response_node)\n",
        "    workflow.add_node(\"store_insights\", store_insights_node)\n",
        "\n",
        "    workflow.set_entry_point(\"load_pdfs\")\n",
        "    workflow.add_edge(\"load_pdfs\", \"analyze_query\")\n",
        "    workflow.add_edge(\"analyze_query\", \"retrieve_info\")\n",
        "    workflow.add_edge(\"retrieve_info\", \"generate_response\")\n",
        "    workflow.add_edge(\"generate_response\", \"store_insights\")\n",
        "    workflow.add_edge(\"store_insights\", END)\n",
        "\n",
        "    compiled = workflow.compile()\n",
        "    return compiled\n",
        "\n",
        "research_app = create_research_graph()\n",
        "print(\"Research graph built successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwlmqrEbaVqR",
        "outputId": "07a38890-848b-4da4-85b3-adff898b7384"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Research graph built successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Cell 7: Colab Upload / Run Example ==========\n",
        "try:\n",
        "    from google.colab import files\n",
        "    is_colab = True\n",
        "except Exception:\n",
        "    is_colab = False\n",
        "\n",
        "if is_colab:\n",
        "    print(\"Running in Colab â€” you can upload PDFs now.\")\n",
        "    uploaded = files.upload()\n",
        "    pdf_paths = list(uploaded.keys())\n",
        "else:\n",
        "    pdf_paths = []  # e.g., [\"./paper1.pdf\"]\n",
        "\n",
        "# Create session_id once and include it in initial_state\n",
        "session_id = str(uuid.uuid4())\n",
        "\n",
        "initial_state: ResearchState = {\n",
        "    \"messages\": [HumanMessage(content=\"What is the main topic of this paper?\")],\n",
        "    \"pdf_paths\": pdf_paths,\n",
        "    \"pdf_chunks\": [],\n",
        "    \"retrieved_docs\": [],\n",
        "    \"long_term_insights\": [],\n",
        "    \"conversation_count\": 0,\n",
        "    \"session_id\": session_id\n",
        "}\n",
        "\n",
        "print(\"Starting session with id:\", session_id)\n",
        "\n",
        "# Run the agent\n",
        "result = research_app.invoke(initial_state)\n",
        "\n",
        "# Show results\n",
        "if \"messages\" in result and result[\"messages\"]:\n",
        "    print(\"Agent Response:\", result[\"messages\"][-1].content)\n",
        "else:\n",
        "    print(\"Agent did not return a response. Check logs above for errors.\")\n",
        "\n",
        "print(\"Retrieved Docs (count):\", len(result.get(\"retrieved_docs\", [])))\n",
        "print(\"Stored Insights:\", result.get(\"long_term_insights\", []))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "4jfm16bWaVn4",
        "outputId": "0c911237-a454-4c39-f67e-f4d6cd55a2c8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Colab â€” you can upload PDFs now.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3e8da604-3e2d-44f5-b656-2475442c938f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3e8da604-3e2d-44f5-b656-2475442c938f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Resume.pdf to Resume.pdf\n",
            "Starting session with id: c3d6dff8-a6bc-4f57-88b0-caa07f0284c1\n",
            "Processed Resume.pdf: 4 chunks\n",
            "Session PDF vectorstore created and cached (via add_texts).\n",
            "load_pdfs_node: loaded 4 chunks across 1 PDF(s). session_id=c3d6dff8-a6bc-4f57-88b0-caa07f0284c1\n",
            "Query analysis: keywords: main topic, paper; intent: to identify the primary subject or focus of the paper\n",
            "Retrieved 3 docs from session PDFs.\n",
            "Retrieved 0 docs from long-term memory.\n",
            "Long-term memory persisted to: /content/long_term_memory\n",
            "Stored 3 insight(s) to long-term memory.\n",
            "Agent Response: Based on the provided context, the main topic of this paper appears to be the development and application of artificial intelligence (AI) and machine learning (ML) technologies, particularly focusing on chatbot systems and AI-driven customer support solutions. The paper discusses projects such as RAG Chatbot from user data, customer support AI agents, Instagram content automation, Telegram chatbots, and text-to-image generation, indicating a broad exploration of AI applications in automation and communication (source: session_pdf_chunk_3).\n",
            "Retrieved Docs (count): 3\n",
            "Stored Insights: ['AI and machine learning are being applied to develop chatbot systems and customer support solutions.', 'The paper explores various AI applications, including content automation, chatbots across platforms, and text-to-image generation.', 'These developments aim to enhance automation and communication efficiency across different domains.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-531324258.py:26: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
            "  long_term_vectorstore.persist()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Cell 8: Short Follow-up demo (short-term memory) ==========\n",
        "if \"messages\" in result and result[\"messages\"]:\n",
        "    ### MODIFIED: Added session_id check and explicit preservation\n",
        "    if not result.get(\"session_id\"):\n",
        "        raise ValueError(\"session_id missing in result state\")\n",
        "    follow_up_state = {\n",
        "        **result,\n",
        "        \"messages\": result[\"messages\"] + [HumanMessage(content=\"Explain the key findings in a short paragraph.\")],\n",
        "        \"session_id\": result[\"session_id\"]  # Explicitly ensure session_id is preserved\n",
        "    }\n",
        "    follow_up_result = research_app.invoke(follow_up_state)\n",
        "    print(\"\\nFollow-up Response:\", follow_up_result[\"messages\"][-1].content)\n",
        "else:\n",
        "    print(\"No messages in result; skipping follow-up.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfIkdzeZaVk4",
        "outputId": "deeba13f-5d50-4491-fd9d-495d94a9fc7f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Resume.pdf: 4 chunks\n",
            "Using pre-existing session vectorstore from cache.\n",
            "load_pdfs_node: loaded 4 chunks across 1 PDF(s). session_id=c3d6dff8-a6bc-4f57-88b0-caa07f0284c1\n",
            "Query analysis: keywords: key findings, short paragraph; intent: request for a summary of main results in a brief paragraph\n",
            "Retrieved 3 docs from session PDFs.\n",
            "Retrieved 3 docs from long-term memory.\n",
            "Long-term memory persisted to: /content/long_term_memory\n",
            "Stored 3 insight(s) to long-term memory.\n",
            "\n",
            "Follow-up Response: The key findings of the paper highlight significant advancements in AI and machine learning applications, particularly in developing chatbot systems and automation tools across various platforms. The projects demonstrate successful implementation of AI-driven solutions such as RAG chatbots, customer support agents, content automation for social media, and text-to-image generation, all contributing to improved efficiency and innovation in communication and service delivery. The results indicate strong performance, with high achievement scores (e.g., 3.56 out of 4.0 and 5.0 out of 5.0), underscoring the effectiveness of these AI applications in solving complex problems and enhancing user engagement (source: session_pdf_chunk_3).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Cell 9: Debug helper & session cleanup ==========\n",
        "def debug_state(state: ResearchState):\n",
        "    ### MODIFIED: Improved session_id reporting and cache check\n",
        "    session_id = state.get(\"session_id\", \"Not set\")\n",
        "    print(\"Debug State:\")\n",
        "    print(f\"  session_id: {session_id}\")\n",
        "    print(f\"  Messages: {len(state.get('messages', []))}\")\n",
        "    print(f\"  PDF Paths: {state.get('pdf_paths', [])}\")\n",
        "    print(f\"  PDF Chunks: {len(state.get('pdf_chunks', []))} chunks\")\n",
        "    print(f\"  Retrieved Docs: {len(state.get('retrieved_docs', []))}\")\n",
        "    print(f\"  Long-Term Insights: {len(state.get('long_term_insights', []))}\")\n",
        "    print(f\"  session_pdf_store_cached: {session_id in SESSION_PDF_STORES and SESSION_PDF_STORES[session_id] is not None}\")\n",
        "    print(f\"  Conversation Count: {state.get('conversation_count', 0)}\")\n",
        "\n",
        "def cleanup_session(session_id: str):\n",
        "    if session_id in SESSION_PDF_STORES:\n",
        "        try:\n",
        "            del SESSION_PDF_STORES[session_id]\n",
        "            print(f\"Session cache {session_id} removed.\")\n",
        "        except Exception as e:\n",
        "            print(\"Error cleaning session cache:\", e)\n",
        "\n",
        "# Inspect the returned state\n",
        "debug_state(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0NddScEatpk",
        "outputId": "4bf38ff0-c2b3-4182-8b2c-9837f2c48b69"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug State:\n",
            "  session_id: c3d6dff8-a6bc-4f57-88b0-caa07f0284c1\n",
            "  Messages: 2\n",
            "  PDF Paths: ['Resume.pdf']\n",
            "  PDF Chunks: 4 chunks\n",
            "  Retrieved Docs: 3\n",
            "  Long-Term Insights: 3\n",
            "  session_pdf_store_cached: True\n",
            "  Conversation Count: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= Cell 10: Ask another query ==========\n",
        "# Check if result has messages and session_id before proceeding\n",
        "if \"messages\" in result and result[\"messages\"] and result.get(\"session_id\"):\n",
        "    new_query_state = {\n",
        "        **result,  # Carry forward the previous state (includes memory & session_id)\n",
        "        \"messages\": result[\"messages\"] + [HumanMessage(content=\"Summarize this paper in 3 bullet points.\")],\n",
        "        \"session_id\": result[\"session_id\"]  # Explicitly preserve session_id\n",
        "    }\n",
        "    result2 = research_app.invoke(new_query_state)\n",
        "    print(\"Agent Response:\", result2[\"messages\"][-1].content)\n",
        "    print(\"Retrieved Docs (count):\", len(result2.get(\"retrieved_docs\", [])))\n",
        "    print(\"Stored Insights:\", result2.get(\"long_term_insights\", []))\n",
        "else:\n",
        "    print(\"Error: Cannot proceed with Cell 8. Result state is missing messages or session_id.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fZ3wdQiatnO",
        "outputId": "029959d9-1e24-438d-f7f6-4eca9b3f2763"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Resume.pdf: 4 chunks\n",
            "Using pre-existing session vectorstore from cache.\n",
            "load_pdfs_node: loaded 4 chunks across 1 PDF(s). session_id=c3d6dff8-a6bc-4f57-88b0-caa07f0284c1\n",
            "Query analysis: keywords: paper, summarize, bullet points; intent: request a concise summary of a paper in three bullet points\n",
            "Retrieved 3 docs from session PDFs.\n",
            "Retrieved 3 docs from long-term memory.\n",
            "Long-term memory persisted to: /content/long_term_memory\n",
            "Stored 3 insight(s) to long-term memory.\n",
            "Agent Response: - The paper highlights various AI applications, including chatbot development (e.g., RAG Chatbot, Telegram bots), customer support AI agents, and content automation tools across platforms like Instagram, demonstrating advancements in automation and user engagement (source: session_pdf_chunk_3).\n",
            "\n",
            "- It emphasizes the effectiveness of these AI solutions, with high performance scores (e.g., 5.0 out of 5.0), showcasing their success in solving complex problems and fostering innovation in digital communication (source: session_pdf_chunk_3).\n",
            "\n",
            "- The research underscores the integration of diverse AI techniques such as natural language processing, text-to-image generation, and open AI SDKs, reflecting a comprehensive approach to enhancing automation, content creation, and interactive systems (source: session_pdf_chunk_3).\n",
            "Retrieved Docs (count): 6\n",
            "Stored Insights: ['- AI applications like chatbots and content automation tools are advancing user engagement across platforms such as Instagram and Telegram.', '- High performance scores indicate the effectiveness of these AI solutions in solving complex problems and driving innovation.', '- The integration of NLP, text-to-image generation, and open AI SDKs demonstrates a comprehensive approach to enhancing automation and content creation.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "145s28_RatkI"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}